{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title << Setup Google Colab by running this cell {display-mode: \"form\"}\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    # Clone GitHub repository\n",
    "    !git clone https://github.com/pxydi/Authorship-analysis.git\n",
    "        \n",
    "    # Copy files required to run the code\n",
    "    #!cp -r \"text/data\" \"text/tools.py\" .\n",
    "    \n",
    "    # Install packages via pip\n",
    "    !pip install -r \"Authorship-analysis/colab-requirements.txt\"\n",
    "    \n",
    "    # Restart Runtime\n",
    "    import os\n",
    "    os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authorship verification\n",
    "\n",
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data source: https://archive.ics.uci.edu/ml/datasets/Victorian+Era+Authorship+Attribution\n",
    "\n",
    "***Documentation***\n",
    "  \n",
    "**Gdelt dataset**  \n",
    "\n",
    "To decrease the bias and create a reliable authorship attribution dataset the following criteria have been chosen to filter out authors in `Gdelt` database: \n",
    "* English language writing authors\n",
    "* Authors that have enough books available (at least 5)\n",
    "* 19th century authors. \n",
    "\n",
    "With these criteria 50 authors have been selected and their books were queried through Big Query Gdelt database. \n",
    "\n",
    "The next task has been cleaning the dataset due to OCR reading problems in the original raw form. To achieve that, \n",
    "* firstly all books have been scanned through to get the overall number of unique words and each words frequencies. \n",
    "* While scanning the texts, the first 500 words and the last 500 words have been removed to take out specific features such as the name of the author, the name of the book and other word specific features that could make the classification task easier. \n",
    "\n",
    "After this step, we have chosen top 10,000 words that occurred in the whole 50 authors text data corpus. The words that are not in top 10,000 words were removed while keeping the rest of the sentence structure intact. Afterwards, the words are represented with numbers from 1 to 10,000 reverse ordered according to their frequencies. \n",
    "\n",
    "The entire book is split into text fragments with 1000 words each. We separately maintained author and book identification number for each one of them in different arrays. \n",
    "\n",
    "Text segments with less than 1000 words were filled with zeros to keep them in the dataset as well. 1000 words make approximately 2 pages of writing, which is long enough to extract a variety of features from the document. The reason why we have represented top 10,000 words with numbers is to keep the anonymity of texts and allow researchers to run feature extraction techniques faster. Dealing with large amounts of text data can be more challenging than numerical data for some feature extraction techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os ,re, random\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import defaultdict\n",
    "from math import ceil\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Load train data\n",
    "\n",
    "path = os.path.join('data','Gungor_2018_VictorianAuthorAttribution_data-train.csv')\n",
    "df = pd.read_csv(path, encoding = \"ISO-8859-1\")\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data\n",
    "\n",
    "df = pd.read_csv('/content/gdrive/My Drive/data/Gungor_2018_VictorianAuthorAttribution_data-train.csv',encoding = \"ISO-8859-1\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a few samples\n",
    "\n",
    "pd.set_option('display.max_colwidth',500)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few observations (train data):\n",
    "* labeled text data\n",
    "* lowercase\n",
    "* no punctuation marks\n",
    "* stopwords aren't removed\n",
    "* a few non-ASCII characters.\n",
    "\n",
    "All texts have a length of 1'000 (words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nbr of samples per author ID\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "sns.barplot(x=pd.value_counts(df['author']).index, y = pd.value_counts(df['author']).values, color='b');\n",
    "plt.xlabel('Author ID number',fontsize=12)\n",
    "plt.ylabel('Number of samples',fontsize=12);\n",
    "plt.title('Number of samples per author',fontsize=14,fontweight='bold');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} unique author ID numbers.'.format(df['author'].nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can see a potential issue here: author ID numbers span from 1 to 50; whereas, there are only 45 unique ID numbers. For the machine learning part, the target labels need to be encoded with values between 0 and n_classes-1. [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) can be used to normalize labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show an example\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create toy labels\n",
    "\n",
    "y = [0, 2, 4]\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y)\n",
    "\n",
    "# Transform labels\n",
    "label_encoder.transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show NLTK's stopwords list\n",
    "from nltk.corpus import stopwords          \n",
    "stopwords_english = stopwords.words('english') \n",
    "print(stopwords_english)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate what type of stopwords we have in the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a document randomly\n",
    "rdn_idx = random.randint(0,len(df)-1)\n",
    "sample = df.iloc[rdn_idx,0]\n",
    "\n",
    "# Empty dictionary\n",
    "stopwords_dict = defaultdict(int)\n",
    "\n",
    "for w in sample.split():\n",
    "    if w in stopwords_english:\n",
    "        stopwords_dict[w] += 1\n",
    "\n",
    "print(stopwords_dict[\"you're\"])\n",
    "print(stopwords_dict[\"don't\"])\n",
    "print(stopwords_dict[\"hadn'\"])\n",
    "print(stopwords_dict[\"not\"])\n",
    "print(stopwords_dict[\"nor\"])\n",
    "print(stopwords_dict[\"no\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it seem that I don't need to expand contractions. I could customize stopwords list not to remove negation words, but, I'm not sure how importart negation is for authorship detection. Anyway, I will use a customized stopwords list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize nltk stopwords\n",
    "not_stopwords = {'no', 'nor', 'not'} \n",
    "custom_stopwords = set([word for word in stopwords_english if word not in not_stopwords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning\n",
    "\n",
    "The text is clean... I will only:\n",
    "* remove non-ASCII characters\n",
    "* apply stemming (to reduce the size of the vocabulary : this may be an issue given the size of the corpus)\n",
    "* remove stopwords\n",
    "* and tokenize (split on the whitespace - no need for something more elaborate). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a sample\n",
    "\n",
    "pd.set_option('display.max_colwidth',None)\n",
    "df.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text is clean... I will only:\n",
    "* remove non-ASCII characters\n",
    "* apply stemming\n",
    "* remove stopwords\n",
    "* and tokenize (split on the whitespace - no need for something more elaborate). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define process_text function\n",
    "\n",
    "def process_text(text):\n",
    "    \n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    # Remove non-ASCII chars\n",
    "    text = ''.join([c for c in str(text) if ord(c) < 128])\n",
    "    \n",
    "    # Tokenize, stem, remove stopwords and 1-char words\n",
    "    clean_tokens = [stemmer.stem(tok) for tok in text.split() if (tok not in custom_stopwords) and (len(tok)>1)]\n",
    "    \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a document randomly\n",
    "\n",
    "rdn_idx = random.randint(0,len(df)-1)\n",
    "sample = df.iloc[rdn_idx,0]\n",
    "\n",
    "print('Before cleaning: \\t{}\\n'.format(sample[0:500]))\n",
    "print('After cleaning: \\t{}\\n'.format(process_text(sample)[0:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Apply process_text function to the entire dataset\n",
    "\n",
    "df['process_text'] = df['text'].apply(lambda x: process_text(x))\n",
    "\n",
    "# Wall time: 6min 13s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-order columns (for convenience)\n",
    "\n",
    "df = df[['text','process_text','author']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few samples\n",
    "\n",
    "pd.set_option('display.max_colwidth',500)\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of words in cleaned texts\n",
    "\n",
    "sns.distplot(df['process_text'].apply(lambda x:len(x)))\n",
    "plt.xlabel('Number of words in samples')\n",
    "plt.ylabel('Frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm curious why some texts have less than 300 words after cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['process_text'].apply(lambda x:len(x)) <  300)].sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks there is an issue with the encoding of a few texts. I don't know how to fix this... So, for the moment, I will remove texts with less than 350 words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df = df.loc[df['process_text'].apply(lambda x:len(x)) >  350].copy()\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of words in cleaned texts (again)\n",
    "\n",
    "sns.distplot(df['process_text'].apply(lambda x:len(x)))\n",
    "plt.xlabel('Number of words in samples')\n",
    "plt.ylabel('Frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export clean data\n",
    "\n",
    "#df.to_csv('data/gdelt_train_clean.csv',index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text vectorization\n",
    "\n",
    "- Build the vocabulary (consider only words that appear in at least 2 documents)\n",
    "- Assign a unique integer to each word of the vocabulary.\n",
    "- Create vector representations of documents. For example:\n",
    "\n",
    "$$\\text{'I came to Bern by train'}$$\n",
    "$$\\text{[700, 680, 320, 230, 120, 55]}$$\n",
    "\n",
    "- Texts aren't of the same size. However, the input to nns, needs to be of fixed size. Therefore, we need to decide what the max length of these vectors should be; (padding or truncation).\n",
    "\n",
    "Let's have a look at the length of the processed texts once more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['process_text'].apply(lambda x:len(x)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using: max_length = 500 seems a good starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X array\n",
    "X = df['process_text']\n",
    "\n",
    "# Create y labels\n",
    "y = df['author']\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Transform labels\n",
    "y = label_encoder.fit_transform(y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train-text split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2)\n",
    "\n",
    "X_train = X_train.values\n",
    "y_train = y_train\n",
    "X_test  = X_test.values\n",
    "y_test  = y_test\n",
    "\n",
    "print('Train data: {} {}'.format(X_train.shape,y_train.shape))\n",
    "print('Test data: {} {}'.format(X_test.shape,y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the vocabulary\n",
    "\n",
    "Now build the vocabulary using the training data and map a unique integer to each word in the vocabulary (we are not counting frequencies!).\n",
    "\n",
    "- Map each word in each text to an integer (an \"index\"). \n",
    "- The following code does this for you, but please read it and understand what it's doing.\n",
    "- Note that you will build the vocabulary based on the training data. \n",
    "- To do so, you will assign an index to everyword by iterating over your training set.\n",
    "\n",
    "The vocabulary will also include some special tokens\n",
    "- `__PAD__`: padding\n",
    "- `</e>`: end of line\n",
    "- `__UNK__`: a token representing any word that is not in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include special tokens \n",
    "# started with pad, end of line and unk tokens\n",
    "Vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2} \n",
    "\n",
    "for text in X_train:\n",
    "    for w in text:\n",
    "        if w not in Vocab:\n",
    "            Vocab[w] = len(Vocab)\n",
    "            \n",
    "print(\"Total words in vocab are\",len(Vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert texts to tensors\n",
    "\n",
    "def doc2tensor(list_tokens, vocab, max_length = 500):\n",
    "    \n",
    "    \"\"\" Converts list of tokens into list of integers\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    list_tokens : list of strings e.g. ['i','came','to','bern','by','train'] \n",
    "        List of tokens to convert to tensor \n",
    "        \n",
    "    vocab : dict\n",
    "        Dictionary mapping each word of the vocabulary to a unique integer\n",
    "        \n",
    "    max_length : integer, default = 500\n",
    "        Output length of document tensor\n",
    "        \n",
    "    Output\n",
    "    ------\n",
    "    \n",
    "    doc_tensor: list of integers (e.g. [700, 680, 320, 230, 120, 55])\n",
    "        Padding and concatenation\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    'I came to Bern by train' -> [700, 680, 320, 230, 120, 55]\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    doc_tensor = []\n",
    "    unk_ID = vocab['__UNK__']\n",
    "    \n",
    "    for tok in list_tokens:\n",
    "        doc_tensor.append(vocab.get(tok,unk_ID))\n",
    "\n",
    "    # If doc_tensor > max_length : concatenation\n",
    "    if len(doc_tensor) > max_length:\n",
    "        doc_tensor = doc_tensor[0:max_length]\n",
    "    # If doc_tensor < max_length : padding      \n",
    "    else:\n",
    "        doc_tensor = doc_tensor + [0]*(max_length - len(doc_tensor))\n",
    "\n",
    "    return doc_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show on a random sample \n",
    "    \n",
    "rdn_idx = random.randint(0,len(X_train)-1)\n",
    "sample = X_train[rdn_idx]\n",
    "\n",
    "print('Before: \\t{}\\n'.format(sample[0:50]))\n",
    "print('After: \\t{}\\n'.format(doc2tensor(sample,Vocab)[0:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensor all dataset\n",
    "\n",
    "# train data\n",
    "X_train_tensor = []\n",
    "\n",
    "for sample in X_train:\n",
    "    X_train_tensor.append(doc2tensor(sample,Vocab))\n",
    "    \n",
    "X_train_tensor = np.array(X_train_tensor)\n",
    "\n",
    "# test data\n",
    "\n",
    "X_test_tensor = []\n",
    "\n",
    "for sample in X_test:\n",
    "    X_test_tensor.append(doc2tensor(sample,Vocab))\n",
    "    \n",
    "X_test_tensor = np.array(X_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few samples\n",
    "\n",
    "X_test_tensor[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Batch generators\n",
    "\n",
    "# ================================== #\n",
    "# Batch generator with shuffling\n",
    "# (for training data)\n",
    "# ================================== #\n",
    "\n",
    "def train_gen(X, y, batch_size):\n",
    "    \n",
    "    # Shuffle X,y\n",
    "    shuffled_idx = np.arange(len(y)) # 1,2,...,n\n",
    "    np.random.shuffle(shuffled_idx)\n",
    "\n",
    "    # Enumerate indexes by steps of batch_size\n",
    "    # i: 0b, 2b, 3b, 4b, .. where b is the batch size\n",
    "    while 1: #run forever, so you can generate elements indefinitely\n",
    "        for i in range(0, len(y), batch_size):\n",
    "            \n",
    "            # Batch indexes\n",
    "            batch_idx = shuffled_idx[i:i+batch_size]\n",
    "\n",
    "            yield X[batch_idx], y[batch_idx]\n",
    "\n",
    "# ================================== #\n",
    "# Batch generator without shuffling\n",
    "# (for val/test data)\n",
    "# ================================== #\n",
    "\n",
    "def test_gen(X, y , batch_size):\n",
    "    # Do not shuffle X,y\n",
    "    idx = np.arange(len(y)) # 1,2,...,n\n",
    "\n",
    "    # Enumerate indexes by steps of batch_size\n",
    "    # i: 0b, 2b, 3b, 4b, .. where b is the batch size\n",
    "    while 1: #run forever, so you can generate elements indefinitely\n",
    "        for i in range(0, len(y), batch_size):\n",
    "            # Batch indexes\n",
    "            batch_idx = idx[i:i+batch_size]\n",
    "\n",
    "            yield X[batch_idx], y[batch_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test train_gen\n",
    "\n",
    "next(train_gen(X_train_tensor,y_train,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test test_gen\n",
    "\n",
    "next(test_gen(X_test_tensor,y_test,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "callbacks = [EarlyStopping(monitor='val_loss',patience=6)]\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import f1_score,accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "\n",
    "maxlen = len(X_train_tensor[0])           # Sequence (document) length\n",
    "embedding_dims = 100                      # Embedding size\n",
    "voc_size = len(Vocab)                     # Size of the vocabulary\n",
    "\n",
    "batch_size = 512\n",
    "epochs = 300\n",
    "\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "print('Max sequence lenght: {}'.format(maxlen))\n",
    "print('Embedding: {}'.format(embedding_dims))\n",
    "print('Vocabulary size: {}'.format(voc_size))\n",
    "print('Batch size: {}'.format(batch_size))\n",
    "print('Number of classes: {}'.format(n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "pd.DataFrame({'label': label_encoder.classes_, 'weight': class_weights}).sort_values(by='weight').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create nn model\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(voc_size, embedding_dims, input_length=maxlen))  # input: (voc_size, embedding_size, seq_length) \n",
    "                                                                         # output: (None, seq_length, emb_size)\n",
    "    model.add(keras.layers.Lambda(lambda x: keras.backend.mean(x, axis=1)))\n",
    "\n",
    "    model.add(Dense(n_classes,activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', \n",
    "                  optimizer = 'adam',\n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "cv = 1\n",
    "\n",
    "for run_idx in range(cv):\n",
    "    \n",
    "    # Create training and validation folds\n",
    "    X_train_tmp, X_val_tmp, y_train_tmp, y_val_tmp =  train_test_split(X_train_tensor,y_train,\n",
    "                                                                       stratify=y_train,\n",
    "                                                                       test_size=0.2,random_state=run_idx)\n",
    "    \n",
    "    print('run_idx: {} - Train size: {} - Val size: {}\\n'.format(run_idx,X_train_tmp.shape,X_val_tmp.shape))\n",
    "\n",
    "    # Create an instance of the nn model\n",
    "    model = create_model()\n",
    "    \n",
    "\n",
    "    # Train model\n",
    "    history = model.fit_generator(generator=train_gen(X_train_tmp, y_train_tmp, batch_size=batch_size),\n",
    "                                  validation_data=test_gen(X_val_tmp,y_val_tmp,batch_size=batch_size), \n",
    "                                  epochs=epochs,\n",
    "                                  steps_per_epoch  = ceil(len(X_train_tmp) / batch_size), # Necessary because using generators\n",
    "                                  validation_steps = ceil(len(X_val_tmp) / batch_size),   # Necessary because using generators\n",
    "                                  callbacks=callbacks,\n",
    "                                  class_weight=class_weights,  # Class weights\n",
    "                                  verbose=1\n",
    "                                 )\n",
    "    \n",
    "    y_train_pred = model.predict_classes(X_train_tmp)\n",
    "    y_val_pred   = model.predict_classes(X_val_tmp)\n",
    "        \n",
    "    print('F1-score -- Train data: {} Validation data: {}'.format(np.round(f1_score(y_train_tmp,y_train_pred,average='micro'),3),\n",
    "                                                                   np.round(f1_score(y_val_tmp, y_val_pred,average='micro'),3)))\n",
    "\n",
    "# Wall time: 1h 3min 9s"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "loss = history.history['loss']\n",
    "acc = history.history['acc']\n",
    "val_loss = history.history['val_loss']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "# Plot loss values\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "\n",
    "ax1.set_title('loss: {:.4f}'.format(val_loss[-1]))\n",
    "ax1.plot(val_loss, label='validation',marker='o')\n",
    "ax1.plot(loss, label='training')\n",
    "ax1.legend()\n",
    "\n",
    "# plot accuracy values\n",
    "ax2.set_title('accuracy: {:.2f}%'.format(val_acc[-1]*100))\n",
    "ax2.plot(val_acc, label='validation')\n",
    "ax2.plot(acc, label='training')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"model0_loss_curves.png\" style=\"width:700px; height:280px;\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.save('model.h5') # Saves architecture and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "K.clear_session() # Create new graph to avoid clutter\n",
    "\n",
    "trained_model_0 = keras.models.load_model('model.h5') # Load model\n",
    "\n",
    "# New predictions\n",
    "y_train_pred_0 = trained_model_0.predict_classes(X_train_tensor)\n",
    "y_test_pred_0  = trained_model_0.predict_classes(X_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train data')\n",
    "print('-'*10)\n",
    "print('F1 score: {}'.format(np.round(f1_score(y_train,y_train_pred_0,average='micro'),3)))\n",
    "print('Accuracy: {}'.format(np.round(accuracy_score(y_train,y_train_pred_0),3)))\n",
    "\n",
    "print()\n",
    "print('Test data')\n",
    "print('-'*10)\n",
    "print('F1 score: {}'.format(np.round(f1_score(y_test,y_test_pred_0,average='micro'),3)))\n",
    "print('Accuracy: {}'.format(np.round(accuracy_score(y_test,y_test_pred_0),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "## TRAIN DATA\n",
    "\n",
    "print(classification_report(y_train, y_train_pred_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST DATA\n",
    "\n",
    "print(classification_report(y_test, y_test_pred_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 60)\n",
    "\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.title('Training data')\n",
    "sns.heatmap(pd.crosstab(y_train,y_train_pred_0),vmax=10,cmap=\"Blues\");\n",
    "plt.ylabel('True labels')\n",
    "plt.xlabel('Predicted labels');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 60)\n",
    "\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.title('Test data')\n",
    "sns.heatmap(pd.crosstab(y_test,y_test_pred_0),vmax=14,cmap=\"Blues\");\n",
    "plt.ylabel('True labels')\n",
    "plt.xlabel('Predicted labels');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will add one dense hidden layer (with 64 neurons). In retrospect, using only 64 neurons may be too low, given the number of output neurons (45).\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create nn model\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "def create_model1():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(voc_size, embedding_dims, input_length=maxlen))  # input: (voc_size, embedding_size, seq_length) \n",
    "                                                                         # output: (None, seq_length, emb_size)\n",
    "    model.add(keras.layers.Lambda(lambda x: keras.backend.mean(x, axis=1)))\n",
    "\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    \n",
    "    model.add(Dense(n_classes,activation='softmax'))\n",
    "\n",
    "    #model.summary()\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', \n",
    "                  optimizer = 'adam',\n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "    return model\n",
    "    \n",
    "model1 = create_model1()\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "cv = 1\n",
    "\n",
    "for run_idx in range(cv):\n",
    "    \n",
    "    # Create training and validation folds\n",
    "    X_train_tmp, X_val_tmp, y_train_tmp, y_val_tmp =  train_test_split(X_train_tensor,y_train,\n",
    "                                                                       stratify=y_train,\n",
    "                                                                       test_size=0.2,random_state=run_idx)\n",
    "    \n",
    "    print('run_idx: {} - Train size: {} - Val size: {}\\n'.format(run_idx,X_train_tmp.shape,X_val_tmp.shape))\n",
    "\n",
    "    # Create an instance of the nn model\n",
    "    model1 = create_model1()\n",
    "    \n",
    "    # Train model\n",
    "    history = model1.fit_generator(generator=train_gen(X_train_tmp, y_train_tmp, batch_size=batch_size),\n",
    "                                  validation_data=test_gen(X_val_tmp,y_val_tmp,batch_size=batch_size), \n",
    "                                  epochs=epochs,\n",
    "                                  steps_per_epoch  = ceil(len(X_train_tmp) / batch_size), # Necessary because using generators\n",
    "                                  validation_steps = ceil(len(X_val_tmp) / batch_size),   # Necessary because using generators\n",
    "                                  callbacks=callbacks,\n",
    "                                  class_weight=class_weights,  # Class weights\n",
    "                                  verbose=2\n",
    "                                 )\n",
    "    \n",
    "    y_train_pred = model1.predict_classes(X_train_tmp)\n",
    "    y_val_pred   = model1.predict_classes(X_val_tmp)\n",
    "        \n",
    "    # Save model and its performance on train/val sets    \n",
    "    print('F1-score -- Train data: {} Validation data: {}'.format(np.round(f1_score(y_train_tmp,y_train_pred,average='micro'),3),\n",
    "                                                                   np.round(f1_score(y_val_tmp, y_val_pred,average='micro'),3)))\n",
    "    print()\n",
    "\n",
    "# Wall time: 19min 18s"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "val_acc = history.history['val_acc']\n",
    "acc = history.history['acc']\n",
    "\n",
    "# Plot loss values\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "\n",
    "ax1.set_title('loss: {:.4f}'.format(val_loss[-1]))\n",
    "ax1.plot(val_loss, label='validation',marker='o')\n",
    "ax1.plot(loss, label='training')\n",
    "ax1.legend()\n",
    "\n",
    "# plot accuracy values\n",
    "ax2.set_title('accuracy: {:.2f}%'.format(val_acc[-1]*100))\n",
    "ax2.plot(val_acc, label='validation')\n",
    "ax2.plot(acc, label='training')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"model1_loss_curves.png\" style=\"width:700px; height:280px;\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model1.save('model1.h5') # Saves architecture and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "K.clear_session() # Create new graph to avoid clutter\n",
    "\n",
    "trained_model_1 = keras.models.load_model('model1.h5') # Load model\n",
    "\n",
    "# New predictions\n",
    "y_train_pred = trained_model_1.predict_classes(X_train_tensor)\n",
    "y_test_pred  = trained_model_1.predict_classes(X_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print('Train data')\n",
    "print('-'*10)\n",
    "print('F1 score: {}'.format(np.round(f1_score(y_train,y_train_pred,average='micro'),3)))\n",
    "print('Accuracy: {}'.format(np.round(accuracy_score(y_train,y_train_pred),3)))\n",
    "\n",
    "print()\n",
    "print('Test data')\n",
    "print('-'*10)\n",
    "print('F1 score: {}'.format(np.round(f1_score(y_test,y_test_pred,average='micro'),3)))\n",
    "print('Accuracy: {}'.format(np.round(accuracy_score(y_test,y_test_pred),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "## TRAIN DATA\n",
    "\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST DATA\n",
    "\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 60)\n",
    "\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.title('Training data')\n",
    "sns.heatmap(pd.crosstab(y_train,y_train_pred),vmax=10,cmap=\"Blues\");\n",
    "plt.ylabel('True labels')\n",
    "plt.xlabel('Predicted labels');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 60)\n",
    "\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.title('Test data')\n",
    "sns.heatmap(pd.crosstab(y_test,y_test_pred),vmax=14,cmap=\"Blues\");\n",
    "plt.ylabel('True labels')\n",
    "plt.xlabel('Predicted labels');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoKeras\n",
    "\n",
    "Source : https://autokeras.com/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#!pip3 install autokeras\n",
    "import autokeras as ak"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_auto = df['text']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train_auto, X_test_auto, y_train, y_test = train_test_split(X_auto, y, stratify=y, test_size=0.2)\n",
    "\n",
    "X_train_auto = X_train_auto.values\n",
    "y_train = y_train\n",
    "X_test_auto  = X_test_auto.values\n",
    "y_test  = y_test\n",
    "\n",
    "print('Train data: {} {}'.format(X_train_auto.shape,y_train.shape))\n",
    "print('Test data: {} {}'.format(X_test_auto.shape,y_test.shape))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Initialize the text classifier\n",
    "auto_keras = ak.TextClassifier(multi_label=True, \n",
    "                               overwrite=True,\n",
    "                               seed = 1,\n",
    "                               objective=\"val_accuracy\",\n",
    "                               max_trials=1)  \n",
    "\n",
    "# Feed the text classifier with training data\n",
    "auto_keras.fit(X_train_auto, y_train, epochs=5) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
